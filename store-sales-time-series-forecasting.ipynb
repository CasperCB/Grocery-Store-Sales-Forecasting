{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Part of this code is adapted from the the following sources:\n# - Kaggle's Time Series Analysis course: https://www.kaggle.com/learn/time-series\n# - Kaggle user FilterJoe's Time Series Bonus Lesson: https://www.kaggle.com/code/filterjoe/time-series-bonus-lesson-unofficial/notebook\n\n# Setup notebook\nfrom warnings import simplefilter\nfrom pathlib import Path\nimport datetime\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing\nimport matplotlib.pyplot as plt # plotting\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_log_error\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom statsmodels.tsa.deterministic import CalendarFourier, DeterministicProcess # for defining time features\n\nfrom xgboost import XGBRegressor # for hybrid modeling\n\nimport pickle # for saving models\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"412cce97-db76-4584-a059-41368094151c","_cell_guid":"a6d25aa2-765e-4b9f-8023-36e873163a35","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-04-18T15:52:11.283891Z","iopub.execute_input":"2023-04-18T15:52:11.284459Z","iopub.status.idle":"2023-04-18T15:52:11.808328Z","shell.execute_reply.started":"2023-04-18T15:52:11.284404Z","shell.execute_reply":"2023-04-18T15:52:11.806988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define training, validation, and test windows\nfull_train_start_day = datetime.datetime(2017, 1, 1)\nfull_train_end_day = datetime.datetime(2017, 8, 15)\n\ntrain_start_day = full_train_start_day\ntrain_end_day = datetime.datetime(2017, 7, 30)\n\nval_start_day = datetime.datetime(2017, 7, 31)\nval_end_day = datetime.datetime(2017, 8, 15)\n\ntest_start_day = datetime.datetime(2017, 8, 16)\ntest_end_day = datetime.datetime(2017, 8, 31)","metadata":{"execution":{"iopub.status.busy":"2023-04-18T15:52:11.810420Z","iopub.execute_input":"2023-04-18T15:52:11.810758Z","iopub.status.idle":"2023-04-18T15:52:11.818017Z","shell.execute_reply.started":"2023-04-18T15:52:11.810725Z","shell.execute_reply":"2023-04-18T15:52:11.816630Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define models to be used\nmod_1 = LinearRegression() \nmod_2 = XGBRegressor()\n\n#hybrid_forecasting_type = \"direct\" # hybrid version of train/validate but need to comment out lagged features\nhybrid_forecasting_type = \"day_by_day_refit_all_days\" # forecasts one day at a time and fixes results, can use lagged features","metadata":{"execution":{"iopub.status.busy":"2023-04-18T15:52:11.819860Z","iopub.execute_input":"2023-04-18T15:52:11.820303Z","iopub.status.idle":"2023-04-18T15:52:11.831397Z","shell.execute_reply.started":"2023-04-18T15:52:11.820266Z","shell.execute_reply":"2023-04-18T15:52:11.830144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n# load training set\ncomp_dir = Path('../input/store-sales-time-series-forecasting')\nstore_sales = pd.read_csv(\n    comp_dir / 'train.csv',\n    usecols = ['date', 'store_nbr', 'family', 'sales', 'onpromotion'],\n    dtype={\n        'store_nbr': 'category',\n        'family': 'category',\n        'sales': 'float32',\n        'onpromotion': 'uint32'},\n    parse_dates=['date'],\n    infer_datetime_format=True,\n    )\n\nstore_sales['date'] = store_sales.date.dt.to_period('D') # convert dates to pandas period dtype\n\n# fill in missing Christmas days\nm_index = pd.MultiIndex.from_product([store_sales[\"store_nbr\"].unique(),\n                                    store_sales[\"family\"].unique(),\n                                    pd.date_range(start=\"2013-1-1\", end=\"2017-8-15\", freq=\"D\").to_period('D')] # to get missing Christmas Days\n                                    ,names=[\"store_nbr\",\"family\", \"date\"])\nstore_sales = store_sales.set_index([\"store_nbr\",\"family\", \"date\"]).reindex(m_index, fill_value=0).sort_index()\n\n\nstore_sales = store_sales.unstack(['store_nbr', 'family']).fillna(0) \nstore_sales = store_sales.stack(['store_nbr', 'family'])\nstore_sales = store_sales[['sales','onpromotion']] # reorder columns to be in the expected order","metadata":{"_uuid":"f81f922f-1943-47e5-ba67-922169deecb7","_cell_guid":"a988876c-9f4f-43d1-88eb-9a64ba733c66","collapsed":false,"_kg_hide-output":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-04-18T15:52:11.833089Z","iopub.execute_input":"2023-04-18T15:52:11.833552Z","iopub.status.idle":"2023-04-18T15:52:37.887383Z","shell.execute_reply.started":"2023-04-18T15:52:11.833498Z","shell.execute_reply":"2023-04-18T15:52:37.885828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# defining a new feature called old_stores that states whether stores were open at the start of data collection or not\ndf_sales_train = pd.read_csv(comp_dir / 'train.csv', parse_dates=['date'])\ndf_trans = pd.read_csv(comp_dir / 'transactions.csv', parse_dates=['date'])\n\ntotal_daily_sales = (\n    df_sales_train.drop(columns = 'onpromotion')\n    .groupby(['date', 'store_nbr'])\n    .sum()\n    .reset_index())\n    \nstores_temp = pd.merge(df_trans, total_daily_sales, on=['date', 'store_nbr']).drop(columns=\"id\")\n\n    \nold_stores = stores_temp[stores_temp['date'] == '2013-01-02'].store_nbr.values","metadata":{"execution":{"iopub.status.busy":"2023-04-18T15:52:37.890628Z","iopub.execute_input":"2023-04-18T15:52:37.891014Z","iopub.status.idle":"2023-04-18T15:52:40.251292Z","shell.execute_reply.started":"2023-04-18T15:52:37.890977Z","shell.execute_reply":"2023-04-18T15:52:40.249618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load in holiday data\ncomp_dir = Path('../input/store-sales-time-series-forecasting')\n\nholidays_events = pd.read_csv(\n    comp_dir / \"holidays_events.csv\",\n    dtype={\n        'type': 'category',\n        'locale': 'category',\n        'locale_name': 'category',\n        'description': 'category',\n        'transferred': 'bool',},\n    parse_dates=['date'],\n    infer_datetime_format=True,)\n\nholidays_events['date'] = holidays_events['date'].replace({'2013-04-29':pd.to_datetime('2013-03-29')}) # 'Good Friday' mistake correction\nholidays_events = holidays_events.set_index('date').to_period('D').sort_index() # note the sort after Good Friday correction","metadata":{"execution":{"iopub.status.busy":"2023-04-18T15:52:40.253036Z","iopub.execute_input":"2023-04-18T15:52:40.253535Z","iopub.status.idle":"2023-04-18T15:52:40.275193Z","shell.execute_reply.started":"2023-04-18T15:52:40.253483Z","shell.execute_reply":"2023-04-18T15:52:40.273575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create workday calendar, incorporating holidays\n# credit to KDJ2020: https://www.kaggle.com/dkomyagin/simple-ts-ridge-rf\n\ncalendar = pd.DataFrame(index=pd.date_range('2013-01-01', '2017-08-31')).to_period('D')\ncalendar['dofw'] = calendar.index.dayofweek\n\ndf_hev = holidays_events[holidays_events.locale == 'National'] # National level only for simplicity\ndf_hev = df_hev.groupby(df_hev.index).first() # Keep one event only\n\ncalendar['wd'] = True\ncalendar.loc[calendar.dofw > 4, 'wd'] = False\ncalendar = calendar.merge(df_hev, how='left', left_index=True, right_index=True)\n\ncalendar.loc[calendar.type == 'Bridge'  , 'wd'] = False\ncalendar.loc[calendar.type == 'Work Day', 'wd'] = True\ncalendar.loc[calendar.type == 'Transfer', 'wd'] = False\ncalendar.loc[(calendar.type == 'Holiday') & (calendar.transferred == False), 'wd'] = False\ncalendar.loc[(calendar.type == 'Holiday') & (calendar.transferred == True ), 'wd'] = True\n\n# Transferred column True: holiday officially falls on that calendar day, but was moved to another date by the government.\n# type Transfer: day transfer (True) holiday is actually celebrated (i.e. 8/10/17 --> 8/11/17 Primer Grito de Independencia)\n\n# type Bridge: extra days that are added to a holiday (e.g., to extend the break across a long weekend).\n# These are frequently made up by the type Work Day which is a day not normally scheduled for work (e.g., Saturday) that is meant to payback the Bridge.\n\n# type Additional:  days added to a regular calendar holiday such as happens around Christmas","metadata":{"execution":{"iopub.status.busy":"2023-04-18T15:52:40.276727Z","iopub.execute_input":"2023-04-18T15:52:40.277433Z","iopub.status.idle":"2023-04-18T15:52:40.341533Z","shell.execute_reply.started":"2023-04-18T15:52:40.277378Z","shell.execute_reply":"2023-04-18T15:52:40.340409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define BoostedHybrid class to train and predict using both models\n\nclass BoostedHybrid:\n    def __init__(self, model_1, model_2):\n        self.model_1 = model_1\n        self.model_2 = model_2\n        self.y_columns = None\n        self.stack_cols = None\n        self.y_resid = None\n\n    def fit1(self, X_1, y, stack_cols=None):\n        self.model_1.fit(X_1, y) # train model 1\n        y_fit = pd.DataFrame(\n            self.model_1.predict(X_1), # predict from model 1\n            index=X_1.index,\n            columns=y.columns,\n        )\n        self.y_resid = y - y_fit # residuals from model 1, which X2 may want to access to create lag (or other) features\n        self.y_resid = self.y_resid.stack(stack_cols).squeeze()  # wide to long\n        \n    def fit2(self, X_2, first_n_rows_to_ignore, stack_cols=None):\n        self.model_2.fit(X_2.iloc[first_n_rows_to_ignore*1782: , :], self.y_resid.iloc[first_n_rows_to_ignore*1782:]) # Train model_2\n        self.y_columns = y.columns # Save for predict method\n        self.stack_cols = stack_cols # Save for predict method\n\n    def predict(self, X_1, X_2, first_n_rows_to_ignore):\n        y_pred = pd.DataFrame(\n            self.model_1.predict(X_1.iloc[first_n_rows_to_ignore: , :]),\n            index=X_1.iloc[first_n_rows_to_ignore: , :].index,\n            columns=self.y_columns,\n        )\n        y_pred = y_pred.stack(self.stack_cols).squeeze()  # wide to long\n#         display(X_2.iloc[first_n_rows_to_ignore*1782: , :]) # uncomment when debugging\n        y_pred += self.model_2.predict(X_2.iloc[first_n_rows_to_ignore*1782: , :]) # Add model_2 predictions to model_1 predictions\n        return y_pred.unstack(self.stack_cols)","metadata":{"_uuid":"f81f922f-1943-47e5-ba67-922169deecb7","_cell_guid":"a988876c-9f4f-43d1-88eb-9a64ba733c66","collapsed":false,"_kg_hide-output":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-04-18T15:52:40.345151Z","iopub.execute_input":"2023-04-18T15:52:40.346369Z","iopub.status.idle":"2023-04-18T15:52:40.358225Z","shell.execute_reply.started":"2023-04-18T15:52:40.346321Z","shell.execute_reply":"2023-04-18T15:52:40.356895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# function to make time features for hybrid model 1 using DeterministicProcess\ndef make_dp_features(df):\n    y = df.loc[:, 'sales']\n    #fourier_a = CalendarFourier(freq='A', order=4)\n    fourier_m = CalendarFourier(freq='M', order=4)\n    dp = DeterministicProcess(\n        index=y.index,\n        constant=True, \n        order=1, # look for a linear trend\n        seasonal=True, \n        additional_terms=[fourier_m], # add Fourier pairs for modeling seasonality\n        drop=True, # drop linearly dependent terms\n    )\n    return y, dp","metadata":{"execution":{"iopub.status.busy":"2023-04-18T15:52:40.360029Z","iopub.execute_input":"2023-04-18T15:52:40.361311Z","iopub.status.idle":"2023-04-18T15:52:40.378180Z","shell.execute_reply.started":"2023-04-18T15:52:40.361261Z","shell.execute_reply":"2023-04-18T15:52:40.377029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create features for hybrid model 1\n\ndef make_X1_features(df, start_date, end_date, is_test_set=False):\n    if is_test_set:\n        X1 = df.rename_axis('date')\n    else:\n        y, dp = make_dp_features(df)\n        X1 = dp.in_sample() # seasonal (weekly) and Fourier (longer time frame) features are generated using DeterministicProcess\n    \n    \n    # other features:\n    \n    #X1['wage_day'] = (X1.index.day == X1.index.daysinmonth) | (X1.index.day == 15) # wage day features seem better for XGBoost than linear regression\n    #X1['wage_day_lag_1'] = (X1.index.day == 1) | (X1.index.day == 16)\n    X1['NewYear'] = (X1.index.dayofyear == 1)\n    X1['Christmas'] = (X1.index=='2016-12-25') | (X1.index=='2015-12-25') | (X1.index=='2014-12-25') | (X1.index=='2013-12-25')\n    \n    if is_test_set:\n        return X1\n    else:\n        return X1, y, dp","metadata":{"execution":{"iopub.status.busy":"2023-04-18T15:52:40.379799Z","iopub.execute_input":"2023-04-18T15:52:40.380277Z","iopub.status.idle":"2023-04-18T15:52:40.397592Z","shell.execute_reply.started":"2023-04-18T15:52:40.380234Z","shell.execute_reply":"2023-04-18T15:52:40.396504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create feature set X2 for hybrid model 2, including helper functions\n\nmax_lag = 7 # max_lag is needed so BoostedHybrid class knows how many rows to drop that have NaN or incorrect data after creating lag features\n            # should match the number of days for the rolling mean used\n\ndef encode_categoricals(df, columns):\n    le = LabelEncoder()  # from sklearn.preprocessing\n    for col in columns:\n        df[col] = le.fit_transform(df[col])\n    return df\n\ndef make_X2_lags(ts, lags, lead_time=1, name='y', stack_cols=None):\n    ts = ts.unstack(stack_cols)\n    df = pd.concat(\n        {\n            f'{name}_lag_{i}': ts.shift(i, freq=\"D\") # freq adds i extra day(s) to end: only one extra day is needed so rest will be dropped\n            for i in range(lead_time, lags + lead_time)\n        },\n        axis=1)\n    df = df.stack(stack_cols).reset_index()\n    df = encode_categoricals(df, stack_cols)\n    df = df.set_index('date').sort_values(by=stack_cols) # return sorted so can correctly compute rolling means (if desired)\n    return df\n\ndef make_X2_features(df, y_resid):\n    stack_columns = ['store_nbr', 'family']\n    \n    # promo_lag features to try later: \n    shifted_promo_df = make_X2_lags(df.squeeze(), lags=4, name='promo', stack_cols=['store_nbr', 'family'])\n    shifted_promo_df['promo_mean_rolling_7'] = shifted_promo_df['promo_lag_1'].rolling(window=7, center=False).mean()\n    #shifted_promo_df['promo_median_rolling_91'] = shifted_promo_df['promo_lag_1'].rolling(window=91, center=False).median().fillna(method='bfill')\n    #shifted_promo_df['promo_median_rolling_162'] = shifted_promo_df['promo_lag_1'].rolling(window=162, center=False).median().fillna(method='bfill')\n    # for rolling window medians, backfilling seems reasonable as medians shouldn't change too much. Trying min_periods produced wacky (buggy?) results\n    \n    # y_lag features, some features to try later: \n    shifted_y_df = make_X2_lags(y_resid, lags=2, name='y_res', stack_cols=stack_columns)\n    shifted_y_df['y_mean_rolling_7'] = shifted_y_df['y_res_lag_1'].rolling(window=7, center=False).mean()\n    #shifted_y_df['y_mean_rolling_14'] = shifted_y_df['y_res_lag_1'].rolling(window=14, center=False).mean()\n    #shifted_y_df['y_mean_rolling_28'] = shifted_y_df['y_res_lag_1'].rolling(window=28, center=False).mean()\n    #shifted_y_df['y_median_rolling_91'] = shifted_y_df['y_res_lag_1'].rolling(window=91, center=False).median().fillna(method='bfill')\n    #shifted_y_df['y_median_rolling_162'] = shifted_y_df['y_res_lag_1'].rolling(window=162, center=False).median().fillna(method='bfill')\n    \n    # other features\n    df = df.reset_index(stack_columns)\n    X2 = encode_categoricals(df, stack_columns)\n    \n#     X2[\"day_of_m\"] = X2.index.day  # day of month (label encloded) feature for learning seasonality\n#     X2 = encode_categoricals(df, ['day_of_m']) # encoding as categorical has tiny impact with XGBoost\n    X2[\"day_of_w\"] = X2.index.dayofweek # does absolutely nothing alone\n    X2 = encode_categoricals(df, ['day_of_w'])\n    old_stores_strings = list(map(str, old_stores))\n    X2['old'] = X2['store_nbr'].isin(old_stores_strings) # True if store had existing sales prior to training time period   \n    #X2['wage_day'] = (X2.index.day == X2.index.daysinmonth) | (X2.index.day == 15) # is it bad to have this in both X1 AND X2?\n    #X2['wage_day_lag_1'] = (X2.index.day == 1) | (X2.index.day == 16)\n    #X2['promo_mean'] = X2.groupby(['store_nbr', 'family'])['onpromotion'].transform(\"mean\") + 0.000001\n    #X2['promo_ratio'] = X2.onpromotion / (X2.groupby(['store_nbr', 'family'])['onpromotion'].transform(\"mean\") + 0.000001)\n\n    # Can experiment with dropping basic feature but keeping something computed from it (i.e. drop y_res_lag_1 feature, keeping y_mean_rolling_7\n#     shifted_y_df.drop('y_res_lag_1', axis=1, inplace=True)\n#     shifted_promo_df.drop('promo_lag_1', axis=1, inplace=True)\n#     X2.drop('onpromotion', axis=1, inplace=True)\n\n    # if removing all lag features, then comment out the following two lines\n    X2 = X2.merge(shifted_y_df, on=['date', 'store_nbr', 'family'], how='left')\n    X2 = X2.merge(shifted_promo_df, on=['date', 'store_nbr', 'family'], how='left') # merges work if they are last line before return\n    return X2","metadata":{"execution":{"iopub.status.busy":"2023-04-18T15:52:40.399022Z","iopub.execute_input":"2023-04-18T15:52:40.399659Z","iopub.status.idle":"2023-04-18T15:52:40.418271Z","shell.execute_reply.started":"2023-04-18T15:52:40.399617Z","shell.execute_reply":"2023-04-18T15:52:40.416705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%time\n\n# quick test for quickly re-training model\n\n# unstack pivots MultiIndex to 54 x 33 = 1782 y columns\nstore_sales_in_date_range = store_sales.unstack(['store_nbr', 'family']).loc[full_train_start_day:full_train_end_day]\n\nmodel = BoostedHybrid(model_1=mod_1, model_2=mod_2) # Boosted Hybrid\n\nX_1, y, dp = make_X1_features(store_sales_in_date_range, full_train_start_day, full_train_end_day) # preparing X1 for hybrid model 1\nmodel.fit1(X_1, y, stack_cols=['store_nbr', 'family']) # fit1 before make_X2_features, since X2 may want to create lag features from model.y_resid\nX_2 = make_X2_features(store_sales_in_date_range # preparing X2 for hybrid model 2\n                       .drop('sales', axis=1)\n                       .stack(['store_nbr', 'family']),\n                       model.y_resid)\nmodel.fit2(X_2, max_lag, stack_cols=['store_nbr', 'family'])\n\ny_pred = model.predict(X_1, X_2, max_lag).clip(0.0)","metadata":{"execution":{"iopub.status.busy":"2023-04-18T15:52:40.420073Z","iopub.execute_input":"2023-04-18T15:52:40.421570Z","iopub.status.idle":"2023-04-18T15:53:15.127079Z","shell.execute_reply.started":"2023-04-18T15:52:40.421516Z","shell.execute_reply":"2023-04-18T15:53:15.125767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train/val split prep that is the same for any of the hybrid forecasting methods\n\ntraining_days = (train_end_day - train_start_day).days + 1\nvalidation_days = (val_end_day - val_start_day).days + 1\nprint(\"Training data set (excluding validation days) has\", training_days, \"days\")\nprint(\"Validation data set has\", validation_days, \"days\\n\")\n\nstore_sales_in_date_range = store_sales.unstack(['store_nbr', 'family']).loc[train_start_day:train_end_day]\nstore_data_in_val_range = store_sales.unstack(['store_nbr', 'family']).loc[val_start_day:val_end_day]\ny_val = y[val_start_day:val_end_day] # use y to evaluate validation set, though we will treat y as unknown when training\n\nmodel_for_val = BoostedHybrid(model_1=mod_1, model_2=mod_2)","metadata":{"execution":{"iopub.status.busy":"2023-04-18T15:53:15.128842Z","iopub.execute_input":"2023-04-18T15:53:15.129217Z","iopub.status.idle":"2023-04-18T15:53:16.565997Z","shell.execute_reply.started":"2023-04-18T15:53:15.129181Z","shell.execute_reply":"2023-04-18T15:53:16.564649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def truncateFloat(data):\n    return tuple( [\"{0:.2f}\".format(x) if isinstance(x,float) else (x if not isinstance(x,tuple) else truncateFloat(x)) for x in data])\n\ntemp = X_2[(X_2.store_nbr == 1) & (X_2.family == 3)]\ntemp.iloc[max_lag: , :].apply(lambda s: truncateFloat(s)) # comment out next line if don't want to see nan rows\n\ntemp.apply(lambda s: truncateFloat(s)).head(10) # note that the fit method of BoostedHybrid class skips over nan rows","metadata":{"execution":{"iopub.status.busy":"2023-04-18T15:53:16.570045Z","iopub.execute_input":"2023-04-18T15:53:16.570449Z","iopub.status.idle":"2023-04-18T15:53:16.629156Z","shell.execute_reply.started":"2023-04-18T15:53:16.570406Z","shell.execute_reply":"2023-04-18T15:53:16.627931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n# DIRECT hybrid version of train/validate (can't use with lagged/rolling features on y, promo, etc.)\nif hybrid_forecasting_type == \"direct\":\n    X_1_train, y_train, dp_val = make_X1_features(store_sales_in_date_range, train_start_day, train_end_day) # preparing X1 for hybrid part 1: LinearRegression\n    model_for_val.fit1(X_1_train, y_train, stack_cols=['store_nbr', 'family']) # fit1 before make_X2_features, since X2 may want to create lag features from model.y_resid\n    X_2_train = make_X2_features(store_sales_in_date_range\n                           .drop('sales', axis=1)\n                           .stack(['store_nbr', 'family']),\n                           model_for_val.y_resid) # preparing X2 for hybrid part 2: XGBoost\n    model_for_val.fit2(X_2_train, max_lag, stack_cols=['store_nbr', 'family'])\n\n    X_1_val = make_X1_features(dp_val.out_of_sample(steps=validation_days), val_start_day, val_end_day, is_test_set=True)\n    X_2_val = make_X2_features(store_data_in_val_range\n                                .drop('sales', axis=1)\n                                .stack(['store_nbr', 'family']),\n                                model_for_val.y_resid) # preparing X2 for hybrid part 2: XGBoost\n    y_fit = model_for_val.predict(X_1_train, X_2_train, max_lag).clip(0.0)\n    y_pred = model_for_val.predict(X_1_val, X_2_val, 0).clip(0.0) # set max_lag to 0 because need entire time span for validation data set\n    \n    if type(model_for_val.model_2) == XGBRegressor:\n        pickle.dump(model_for_val.model_2, open(\"xgb_temp.pkl\", \"wb\"))\n        m2 = pickle.load(open(\"xgb_temp.pkl\", \"rb\"))\n        print(\"XGBRegressor paramaters:\\n\",m2.get_xgb_params(), \"\\n\")","metadata":{"execution":{"iopub.status.busy":"2023-04-18T15:53:16.631111Z","iopub.execute_input":"2023-04-18T15:53:16.631486Z","iopub.status.idle":"2023-04-18T15:53:16.640571Z","shell.execute_reply.started":"2023-04-18T15:53:16.631449Z","shell.execute_reply":"2023-04-18T15:53:16.639283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n# forecast 1 day at a time which allows use of lag features. Slow.\n# Each new day y is fixed after it's initial prediction - no y row is ever predicted more than once\nif hybrid_forecasting_type == \"day_by_day_fixed_past\":\n    #initial fit on train portion of train/val split\n    X_1_train, y_train, dp_val = make_X1_features(store_sales_in_date_range, train_start_day, train_end_day) # preparing X1 for hybrid part 1: LinearRegression\n    model_for_val.fit1(X_1_train, y_train, stack_cols=['store_nbr', 'family']) # fit1 before make_X2_features, since X2 may want to create lag features from model.y_resid\n    X_2_train = make_X2_features(store_sales_in_date_range\n                           .drop('sales', axis=1)\n                           .stack(['store_nbr', 'family']),\n                           model_for_val.y_resid) # preparing X2 for hybrid part 2: XGBoost\n    model_for_val.fit2(X_2_train, max_lag, stack_cols=['store_nbr', 'family'])\n    y_fit = model_for_val.predict(X_1_train, X_2_train, max_lag).clip(0.0)\n\n    y_pred_combined = y_fit.copy() # initialize y_pred_combined\n\n    # loop through forecast, one day (\"step\") at a time\n    dp_for_full_X1_val_date_range = dp_val.out_of_sample(steps=validation_days)\n    for step in range(validation_days):\n        dp_steps_so_far = dp_for_full_X1_val_date_range.loc[val_start_day:val_start_day+pd.Timedelta(days=step),:]\n\n        X_1_combined_dp_data = pd.concat([dp_val.in_sample(), dp_steps_so_far])\n        X_2_combined_data = pd.concat([store_sales_in_date_range,\n                                       store_data_in_val_range.loc[val_start_day:val_start_day+pd.Timedelta(days=step), :]])\n        X_1_val = make_X1_features(X_1_combined_dp_data, train_start_day, val_start_day+pd.Timedelta(days=step), is_test_set=True)\n        X_2_val = make_X2_features(X_2_combined_data\n                                    .drop('sales', axis=1)\n                                    .stack(['store_nbr', 'family']),\n                                    model_for_val.y_resid) # preparing X2 for hybrid part 2: XGBoost\n\n    #     print(\"last 3 rows of X_1_val: \")\n    #     display(X_1_val.tail(3))\n    #     temp_val2 = X_2_val[(X_2_val.store_nbr == 1) & (X_2_val.family == 3)]\n    #     print(\"last 3 rows of X_2_val: \")\n    #     display(temp_val2.tail(3).apply(lambda s: truncateFloat(s)))\n\n        y_pred_combined = pd.concat([y_pred_combined,\n                                     model_for_val.predict(X_1_val, X_2_val, max_lag).clip(0.0).iloc[-1:]\n                                    ])\n    #     print(\"last 3 rows of y_combined: \")\n    #     display(y_pred_combined.tail(3).apply(lambda s: truncateFloat(s)))\n        y_plus_y_val = pd.concat([y_train, y_pred_combined.iloc[-(step+1):]]) # add newly predicted rows of y_pred_combined\n        model_for_val.fit1(X_1_val, y_plus_y_val, stack_cols=['store_nbr', 'family']) # fit on new combined X, y - note that fit prior to val date range will change slightly\n        model_for_val.fit2(X_2_val, max_lag, stack_cols=['store_nbr', 'family'])\n\n        rmsle_valid = mean_squared_log_error(y_val.iloc[step:step+1], y_pred_combined.iloc[-1:]) ** 0.5\n        print(f'Validation RMSLE: {rmsle_valid:.5f}', \"for\", val_start_day+pd.Timedelta(days=step))\n\n    y_pred = y_pred_combined[val_start_day:val_end_day]\n    print(\"\\ny_pred: \")\n    display(y_pred.apply(lambda s: truncateFloat(s)))\n    \n    if type(model_for_val.model_2) == XGBRegressor:\n        pickle.dump(model_for_val.model_2, open(\"xgb_temp.pkl\", \"wb\"))\n        m2 = pickle.load(open(\"xgb_temp.pkl\", \"rb\"))\n        print(\"XGBRegressor paramaters:\\n\",m2.get_xgb_params(), \"\\n\")","metadata":{"execution":{"iopub.status.busy":"2023-04-18T15:53:16.642460Z","iopub.execute_input":"2023-04-18T15:53:16.642914Z","iopub.status.idle":"2023-04-18T15:53:16.665709Z","shell.execute_reply.started":"2023-04-18T15:53:16.642874Z","shell.execute_reply":"2023-04-18T15:53:16.664306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# forecast 1 day at a time which allows use of lag features. Slow.\n# each new forecast causes y for all days in training and test (or validation set) to be reforecast.\nif hybrid_forecasting_type == \"day_by_day_refit_all_days\":\n    #initial fit on train portion of train/val split\n    X_1_train, y_train, dp_val = make_X1_features(store_sales_in_date_range, train_start_day, train_end_day) # preparing X1 for hybrid part 1: LinearRegression\n    model_for_val.fit1(X_1_train, y_train, stack_cols=['store_nbr', 'family']) # fit1 before make_X2_features, since X2 may want to create lag features from model.y_resid\n    X_2_train = make_X2_features(store_sales_in_date_range\n                           .drop('sales', axis=1)\n                           .stack(['store_nbr', 'family']),\n                           model_for_val.y_resid) # preparing X2 for hybrid part 2: XGBoost\n    model_for_val.fit2(X_2_train, max_lag, stack_cols=['store_nbr', 'family'])\n    y_fit = model_for_val.predict(X_1_train, X_2_train, max_lag).clip(0.0)\n\n    # loop through forecast, one day (\"step\") at a time\n    dp_for_full_X1_val_date_range = dp_val.out_of_sample(steps=validation_days)\n    for step in range(validation_days):\n        dp_steps_so_far = dp_for_full_X1_val_date_range.loc[val_start_day:val_start_day+pd.Timedelta(days=step),:]\n\n        X_1_combined_dp_data = pd.concat([dp_val.in_sample(), dp_steps_so_far])\n        X_2_combined_data = pd.concat([store_sales_in_date_range,\n                                       store_data_in_val_range.loc[val_start_day:val_start_day+pd.Timedelta(days=step), :]])\n        X_1_val = make_X1_features(X_1_combined_dp_data, train_start_day, val_start_day+pd.Timedelta(days=step), is_test_set=True)\n        X_2_val = make_X2_features(X_2_combined_data\n                                    .drop('sales', axis=1)\n                                    .stack(['store_nbr', 'family']),\n                                    model_for_val.y_resid) # preparing X2 for hybrid part 2: XGBoost\n\n    #     print(\"last 3 rows of X_1_val: \")\n    #     display(X_1_val.tail(3))\n    #     temp_val2 = X_2_val[(X_2_val.store_nbr == 1) & (X_2_val.family == 3)]\n    #     print(\"last 3 rows of X_2_val: \")\n    #     display(temp_val2.tail(3).apply(lambda s: truncateFloat(s)))\n\n        y_pred_combined = model_for_val.predict(X_1_val, X_2_val, max_lag).clip(0.0) # generate y with \n    #     print(\"last 3 rows of y_combined: \")\n    #     display(y_pred_combined.tail(3).apply(lambda s: truncateFloat(s)))\n        y_plus_y_val = pd.concat([y_train, y_pred_combined.iloc[-(step+1):]]) # add newly predicted rows of y_pred_combined\n        model_for_val.fit1(X_1_val, y_plus_y_val, stack_cols=['store_nbr', 'family']) # fit on new combined X, y - note that fit prior to val date range will change slightly\n        model_for_val.fit2(X_2_val, max_lag, stack_cols=['store_nbr', 'family'])\n\n        rmsle_valid = mean_squared_log_error(y_val.iloc[step:step+1], y_pred_combined.iloc[-1:]) ** 0.5\n        print(f'Validation RMSLE: {rmsle_valid:.5f}', \"for\", val_start_day+pd.Timedelta(days=step))\n    #     print(\"end of round \", step)\n\n    y_pred = y_pred_combined[val_start_day:val_end_day]\n    print(\"\\ny_pred: \")\n    display(y_pred.apply(lambda s: truncateFloat(s)))\n    \n    if type(model_for_val.model_2) == XGBRegressor:\n        pickle.dump(model_for_val.model_2, open(\"xgb_temp.pkl\", \"wb\"))\n        m2 = pickle.load(open(\"xgb_temp.pkl\", \"rb\"))\n        print(\"XGBRegressor paramaters:\\n\",m2.get_xgb_params(), \"\\n\")","metadata":{"execution":{"iopub.status.busy":"2023-04-18T15:53:16.667501Z","iopub.execute_input":"2023-04-18T15:53:16.667932Z","iopub.status.idle":"2023-04-18T16:02:39.282718Z","shell.execute_reply.started":"2023-04-18T15:53:16.667885Z","shell.execute_reply":"2023-04-18T16:02:39.280884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rmsle_train = mean_squared_log_error(y_train.iloc[max_lag: , :].clip(0.0), y_fit) ** 0.5\nrmsle_valid = mean_squared_log_error(y_val.clip(0.0), y_pred) ** 0.5\nprint()\nprint(f'Training RMSLE: {rmsle_train:.5f}')\nprint(f'Validation RMSLE: {rmsle_valid:.5f}')\n    \ny_predict = y_pred.stack(['store_nbr', 'family']).reset_index()\ny_target = y_val.stack(['store_nbr', 'family']).reset_index().copy()\ny_target.rename(columns={y_target.columns[3]:'sales'}, inplace=True)\ny_target['sales_pred'] = y_predict[0].clip(0.0) # Sales should be >= 0\ny_target['store_nbr'] = y_target['store_nbr'].astype(int)\n\nprint('\\nValidation RMSLE by family')\ndisplay(y_target.groupby('family').apply(lambda r: mean_squared_log_error(r['sales'], r['sales_pred'])))\n\nprint('\\nValidation RMSLE by store')\ndisplay(y_target.sort_values(by=\"store_nbr\").groupby('store_nbr').apply(lambda r: mean_squared_log_error(r['sales'], r['sales_pred'])))","metadata":{"execution":{"iopub.status.busy":"2023-04-18T16:02:39.284117Z","iopub.execute_input":"2023-04-18T16:02:39.284468Z","iopub.status.idle":"2023-04-18T16:02:39.503158Z","shell.execute_reply.started":"2023-04-18T16:02:39.284433Z","shell.execute_reply":"2023-04-18T16:02:39.501805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load in the test set\ndf_test = pd.read_csv(\n    comp_dir / 'test.csv',\n    dtype={\n        'store_nbr': 'category',\n        'family': 'category',\n        'onpromotion': 'uint32',\n    },\n    parse_dates=['date'],\n    infer_datetime_format=True,\n)\ndf_test['date'] = df_test.date.dt.to_period('D')\ndf_test = df_test.set_index(['store_nbr', 'family', 'date']).sort_index()","metadata":{"_uuid":"801d504b-4629-451d-9891-08a24241683e","_cell_guid":"73c379e4-393f-412d-af2e-679ae2e0733e","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train/test split prep that is the same for any of the hybrid forecasting methods\ntrain_days = (full_train_end_day - full_train_start_day).days + 1\ntest_days = (test_end_day - test_start_day).days + 1\n\nprint(\"data trained over\", train_days, \"days\")\nprint(\"test forecasting period is\", test_days, \"days through\", test_end_day, \"\\n\")\nstore_sales_in_date_range = store_sales.unstack(['store_nbr', 'family']).loc[full_train_start_day:full_train_end_day]\nstore_data_in_test_range = df_test.unstack(['store_nbr', 'family']).drop('id', axis=1)\n\n# previously prepared data and fit \"model\" from data ranging from full_train_start_day to full_train_end_day. Can be used by when fitting test.\nmodel_for_test = BoostedHybrid(model_1=mod_1, model_2=mod_2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# DIRECT hybrid version of train/validate (can't use with lagged/rolling features on y, promo, etc.)\n# model.fit (1 and 2) already happened previously (on training set with a time range ending on most recent date)\n\nif hybrid_forecasting_type == \"direct\":\n    X_1_test = make_X1_features(dp.out_of_sample(steps=int(test_days)), test_start_day, test_end_day, is_test_set=True)\n    X_2_test = make_X2_features(store_data_in_test_range.loc[test_start_day:test_end_day]\n                                .stack(['store_nbr', 'family']),\n                                model.y_resid) # preparing X2 for hybrid part 2: XGBoost\n    y_forecast = pd.DataFrame(model.predict(X_1_test, X_2_test, 0).clip(0.0), index=X_1_test.index, columns=y.columns) # set max_lag to 0 because need entire time span for test data set\n    \n    if type(model.model_2) == XGBRegressor:\n        pickle.dump(model.model_2, open(\"xgb_temp.pkl\", \"wb\"))\n        m2 = pickle.load(open(\"xgb_temp.pkl\", \"rb\"))\n        print(\"XGBRegressor paramaters:\\n\",m2.get_xgb_params(), \"\\n\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# forecast 1 day at a time which allows use of lag features. Slow.\n# Each new day y is fixed after it's initial prediction - no y row is ever predicted more than once\nif hybrid_forecasting_type == \"day_by_day_fixed_past\":\n    #initial fit on train portion of train/test split\n    X_1_train, y_train, dp_test = make_X1_features(store_sales_in_date_range, full_train_start_day, full_train_end_day) # preparing X1 for hybrid part 1: LinearRegression\n    model_for_test.fit1(X_1_train, y_train, stack_cols=['store_nbr', 'family']) # fit1 before make_X2_features, since X2 may want to create lag features from model.y_resid\n    X_2_train = make_X2_features(store_sales_in_date_range\n                           .drop('sales', axis=1)\n                           .stack(['store_nbr', 'family']),\n                           model_for_test.y_resid) # preparing X2 for hybrid part 2: XGBoost\n    model_for_test.fit2(X_2_train, max_lag, stack_cols=['store_nbr', 'family'])\n\n    y_forecast_combined = model_for_test.predict(X_1_train, X_2_train, max_lag).clip(0.0) # initializing with training set fit\n\n    dp_for_full_X1_test_date_range = dp_test.out_of_sample(steps=test_days)\n    for step in range(test_days):\n        dp_steps_so_far = dp_for_full_X1_test_date_range.loc[test_start_day:test_start_day+pd.Timedelta(days=step),:]\n\n        X_1_combined_dp_data = pd.concat([dp_test.in_sample(), dp_steps_so_far])\n        X_2_combined_data = pd.concat([store_sales_in_date_range,\n                                       store_data_in_test_range.loc[test_start_day:test_start_day+pd.Timedelta(days=step), :]])\n        X_1_test = make_X1_features(X_1_combined_dp_data, train_start_day, test_start_day+pd.Timedelta(days=step), is_test_set=True)\n        X_2_test = make_X2_features(X_2_combined_data\n                                    .drop('sales', axis=1)\n                                    .stack(['store_nbr', 'family']),\n                                    model_for_test.y_resid) # preparing X2 for hybrid part 2: XGBoost\n    #     print(\"last 3 rows of X_1_test: \")\n    #     display(X_1_test.tail(3))\n    #     temp_test2 = X_2_test[(X_2_test.store_nbr == 1) & (X_2_test.family == 3)]\n    #     print(\"last 3 rows of X_2_test: \")\n    #     display(temp_test2.tail(3).apply(lambda s: truncateFloat(s)))\n\n        y_forecast_combined = pd.concat([y_forecast_combined,\n                                     model_for_test.predict(X_1_test, X_2_test, max_lag).clip(0.0).iloc[-1:]\n                                    ])   \n    #     print(\"last 3 rows of y_forecast_combined: \")\n    #     display(y_forecast_combined.tail(3).apply(lambda s: truncateFloat(s)))\n\n        y_plus_y_test = pd.concat([y_train, y_forecast_combined.iloc[-(step+1):]]) # add newly predicted (last step+1) rows of y_test_combined\n        model_for_test.fit1(X_1_test, y_plus_y_test, stack_cols=['store_nbr', 'family']) # fit on new combined X, y - note that fit prior to test date range will change slightly\n        model_for_test.fit2(X_2_test, max_lag, stack_cols=['store_nbr', 'family'])\n        print(\"finished forecast for\", test_start_day+pd.Timedelta(days=step))\n\n    display(y_forecast_combined[test_start_day:test_end_day])\n    y_forecast = pd.DataFrame(y_forecast_combined[test_start_day:test_end_day].clip(0.0), index=X_1_test.index, columns=y.columns)\n    print('\\nFinished creating test set forecast\\n')\n    \n    if type(model_for_test.model_2) == XGBRegressor:\n        pickle.dump(model_for_test.model_2, open(\"xgb_temp.pkl\", \"wb\"))\n        m2 = pickle.load(open(\"xgb_temp.pkl\", \"rb\"))\n        print(\"XGBRegressor paramaters:\\n\",m2.get_xgb_params(), \"\\n\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# forecast 1 day at a time which allows use of lag features. Slow.\n# each new forecast causes y for all days in training and test (or validation set) to be reforecast.\nif hybrid_forecasting_type == \"day_by_day_refit_all_days\":\n    #initial fit on train portion of train/test split\n    X_1_train, y_train, dp_test = make_X1_features(store_sales_in_date_range, full_train_start_day, full_train_end_day) # preparing X1 for hybrid part 1: LinearRegression\n    model_for_test.fit1(X_1_train, y_train, stack_cols=['store_nbr', 'family']) # fit1 before make_X2_features, since X2 may want to create lag features from model.y_resid\n    X_2_train = make_X2_features(store_sales_in_date_range\n                           .drop('sales', axis=1)\n                           .stack(['store_nbr', 'family']),\n                           model_for_test.y_resid) # preparing X2 for hybrid part 2: XGBoost\n    model_for_test.fit2(X_2_train, max_lag, stack_cols=['store_nbr', 'family'])\n    # y_full_train = model_for_test.predict(X_1_train, X_2_train, max_lag).clip(0.0) # do I need this line here?\n\n\n    dp_for_full_X1_test_date_range = dp_test.out_of_sample(steps=test_days)\n    for step in range(test_days):\n        dp_steps_so_far = dp_for_full_X1_test_date_range.loc[test_start_day:test_start_day+pd.Timedelta(days=step),:]\n\n        X_1_combined_dp_data = pd.concat([dp_test.in_sample(), dp_steps_so_far])\n        X_2_combined_data = pd.concat([store_sales_in_date_range,\n                                       store_data_in_test_range.loc[test_start_day:test_start_day+pd.Timedelta(days=step), :]])\n        X_1_test = make_X1_features(X_1_combined_dp_data, train_start_day, test_start_day+pd.Timedelta(days=step), is_test_set=True)\n        X_2_test = make_X2_features(X_2_combined_data\n                                    .drop('sales', axis=1)\n                                    .stack(['store_nbr', 'family']),\n                                    model_for_test.y_resid) # preparing X2 for hybrid part 2: XGBoost\n    #     print(\"last 3 rows of X_1_test: \")\n    #     display(X_1_test.tail(3))\n    #     temp_test2 = X_2_test[(X_2_test.store_nbr == 1) & (X_2_test.family == 3)]\n    #     print(\"last 3 rows of X_2_test: \")\n    #     display(temp_test2.tail(3).apply(lambda s: truncateFloat(s)))\n\n        y_forecast_combined = model_for_test.predict(X_1_test, X_2_test, max_lag).clip(0.0) # generate y with \n\n    #     print(\"last 3 rows of y_forecast_combined: \")\n    #     display(y_forecast_combined.tail(3).apply(lambda s: truncateFloat(s)))\n\n        y_plus_y_test = pd.concat([y_train, y_forecast_combined.iloc[-(step+1):]]) # add newly predicted (last step+1) rows of y_test_combined\n        model_for_test.fit1(X_1_test, y_plus_y_test, stack_cols=['store_nbr', 'family']) # fit on new combined X, y - note that fit prior to test date range will change slightly\n        model_for_test.fit2(X_2_test, max_lag, stack_cols=['store_nbr', 'family'])\n        print(\"finished forecast for\", test_start_day+pd.Timedelta(days=step))\n\n    display(y_forecast_combined[test_start_day:test_end_day])\n\n    y_forecast = pd.DataFrame(y_forecast_combined[test_start_day:test_end_day].clip(0.0), index=X_1_test.index, columns=y.columns)\n    print('\\nFinished creating test set forecast\\n')\n    \n    if type(model_for_test.model_2) == XGBRegressor:\n        pickle.dump(model_for_test.model_2, open(\"xgb_temp.pkl\", \"wb\"))\n        m2 = pickle.load(open(\"xgb_temp.pkl\", \"rb\"))\n        print(\"XGBRegressor paramaters:\\n\",m2.get_xgb_params(), \"\\n\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#X_test = dp.out_of_sample(steps = 16)\n#X_test['NewYearsDay'] = (X_test.index.dayofyear == 1)\n\n#X_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# make predictions for test set\n# will use a dictionary to store id:sales for every test value\n#test_dict = {}\n       \n# generate features for 16-day forecast window\n#X_test = dp.out_of_sample(steps = 16)\n#X_test['NewYearsDay'] = (X_test.index.dayofyear == 1)\n\n# loop over all stores\n#for store_nbr, test_0 in test.groupby(level = 0):\n    \n    # loop over all sales categories\n#    for family, test_1 in test_0.groupby(level = 1):\n        \n        # pick current category\n#        test_family = test_1.index.get_level_values(1)[0]\n    \n        # fetch linear model for detrend & deseason\n#        model_1 = LinearRegression()\n#        model_1.coef_ = model_params[test_family]\n#        model_1.intercept_ = intercepts[test_family]\n#        model_1.feature_names_in_ = X_test.columns\n        \n        # predict sales for forecast window, output is a list\n#        y_test1 = model_1.predict(X_test)\n        \n        # set up features for XGBoost\n        #promo = test_1.onpromotion.reset_index(drop = True) # grab onpromotion data\n        \n#        day = pd.Series(X_test.index.day) # grab 'day' values\n#        X_test_xgb = pd.concat([pd.Series(test_1.index.get_level_values(1)), promo, day], axis = 1,  ignore_index = True) # concatenate the above with current family \n#        X_test_xgb = X_test_xgb.rename(columns = {0:'family', 1:'onpromotion', 2:'day'}) # rename columns\n#        X_test_xgb['family'] = le.transform(X_test_xgb['family']) # encode 'family'\n#        X_test_xgb.index = X_test.index # set index\n\n        # predict residual seasonality and other effects with XGBoost\n#        y_test2 = model_xgb.predict(X_test_xgb)\n        \n        # combine models\n#        y_test = y_test1 + y_test2\n#        y_test = np.where(y_test<0, 0, y_test) # metric is RMSLE, replace negative values to avoid log errors\n        \n        # grab test ids, output is a list\n#        keys = test_1.id.to_string(header = False, index = False).split()\n        \n        # store ids and inferences in a dictionary\n#        for i, key in enumerate(keys):\n#            value = y_test[i]\n#            test_dict[key] = value\n            \n# convert dictionary of inferences to DataFrame to write to .csv\n#test_sub = pd.DataFrame.from_dict(test_dict, orient = 'index', columns = ['sales'])\n#test_sub.index.name = 'id' \n#test_sub = test_sub.sort_index(ascending = True)\n\n#test_sub.to_csv('submission.csv')","metadata":{"_uuid":"45c319ed-731e-4810-99ac-44e060aec107","_cell_guid":"b682ccd1-1e20-44fa-aa42-26fef0a39951","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot style settings from learntools.time_series.style\n\nplt.style.use(\"seaborn-whitegrid\")\nplt.rc(\n    \"figure\",\n    autolayout=True,\n    figsize=(11, 4),\n    titlesize=18,\n    titleweight='bold',\n)\nplt.rc(\n    \"axes\",\n    labelweight=\"bold\",\n    labelsize=\"large\",\n    titleweight=\"bold\",\n    titlesize=16,\n    titlepad=10,\n)\nplot_params = dict(\n    color=\"0.75\",\n    style=\".-\",\n    markeredgecolor=\"0.25\",\n    markerfacecolor=\"0.25\",\n    legend=False,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# see example predictions (both validation and test data sets) for a specific store/family:\n\nSTORE_NBR = '1'  # 1 - 54\nFAMILY = 'BEVERAGES' # display(store_sales.index.get_level_values('family').unique())\n\nax = y.loc(axis=1)[STORE_NBR, FAMILY].plot(**plot_params, figsize=(16, 4))\nax = y_pred.loc(axis=1)[STORE_NBR, FAMILY].plot(ax=ax, marker='.', color='red', markersize=12) # markers: big size for tiny validation sets (1-2 days)\nax = y_forecast.loc(axis=1)[STORE_NBR, FAMILY].plot(ax=ax, marker='.', color='orange', markersize=12)\nax.set_title(f'{FAMILY} Sales at Store {STORE_NBR}');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# creates submission file submission.csv\n\ny_submit = y_forecast.stack(['store_nbr', 'family'])\ny_submit = pd.DataFrame(y_submit, columns=['sales'])\ny_submit = y_submit.join(df_test.id).reindex(columns=['id', 'sales'])\ny_submit.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_submit","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}